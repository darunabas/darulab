<div class="research-info">
<h3>A Daru Lab Guide to Using the Stanford Sherlock HPC Cluster</h3>
    <hr>
<h4>Sherlock HPC Resources</h4>
<p>We have access to the <em>Sherlock</em> clusters. Documentation for Sherlock is <a href="https://www.sherlock.stanford.edu/docs/user-guide/running-jobs/">here</a>. <p>As a member of the Daru lab, you have access to the following resources on Sherlock:</p>
<ul>
    <li>Individual and Group Scratch Space: You have access to 100TB of scratch space for storing temporary data. Please note that files on SCRATCH and GROUP_SCRATCH are automatically purged 90 days after their last content modification. So, make sure to move your data before that time to avoid loss.</li>
    <li>Home Directory: Each lab member has 15GB of space in their home directory.</li>
    <li>GROUP_HOME: This is a shared resource with 1.0TB of space. Before using GROUP_HOME, please contact me for approval.</li>
    <li>Partitions: The partitions available to Daru lab members are the "normal" and "hns" partitions. Each partition has a maximum walltime of 2 days for running jobs.</li>
</ul>

    <style>
        pre {
            background-color: #f0f0f0;
            padding: 10px;
            font-size: 14px;
        }
    </style>
  <h4>Connecting by SSH</h4>
    <p>Use SSH from a terminal and your UNI credentials to log in.</p>
    <pre><code># Connect to Sherlock from your local computer
ssh &lt;user&gt;@login.sherlock.stanford.edu
    </code></pre>

    <h4>Setup Your Scratch Directory</h4>
    <p>On Sherlock, you can access several partitions, but the ones commonly used in the Daru lab are the "hns" and "normal" partitions. Use your user-specific scratch directory to run your jobs. This is also where you can temporarily store large data files, but remember that files on SCRATCH and GROUP_SCRATCH are automatically purged 90 days after their last content modification.</p>
    <p>To transfer files from your local computer to the cluster, you can use <code>sftp</code>, or you can download data directly on the cluster if it is hosted online somewhere.</p>
    <pre><code># On your local computer
# Transfer files or dirs from your local computer to the scratch space
sftp &lt;user&gt;@dtn.sherlock.stanford.edu
put /pathtofile/on/your/computer/file.csv
    </code></pre>

    <h4>Submit Jobs to the Cluster Using SLURM</h4>
    <p>The Sherlock cluster uses the SLURM job submission system to manage shared resources on the cluster. When you log in, you will start at the "head" node, which is like a waiting area. Remember not to run any heavy tasks on this node. Instead, you can submit your jobs using a "job script" to reserve resources for your job and send it to run on a "compute node".</p>
    <p>To keep things organized, create a directory called "Batch" for your job submissions. This way, you can easily keep track of your tasks.</p>
    <pre><code># On the head node
mkdir ~/Batch{1..10}
    </code></pre>

    <h4>Example Job Submission</h4>
    <p>The bash script below tells the scheduler the resources we need, which partition to use ("hns"), and how the job and output files should be named. The command below reserves 16 cores and executes the <code>R CMD BATCH</code> command to run the R script <code>calibration.R</code> in the Batch* folder. The file is named <code>script.sbatch</code> and is placed one directory outside where the R scripts are located in the "Batch/" directory.</p>
    <pre><code># Open file with vi text editor on the head node
vi script.sbatch
    </code></pre>
    <pre><code>#!/usr/bin/bash
#SBATCH --time=2-00
#SBATCH -p hns
#SBATCH --ntasks-per-node=16
#SBATCH --nodes=1
#SBATCH --mem=90GB

cd ${1}

R CMD BATCH --no-save --no-restore calibration.R output.out
    </code></pre>
    <p>Submit the job to the scheduling queue.</p>
    <pre><code># On the head node
for DIR in Batch*; do sbatch script.sbatch $DIR; done
    </code></pre>
    <p>Check whether it has started yet.</p>
    <pre><code># On the head node
sacct
    </code></pre>

    <h4>Interactive Mode</h4>
    <p>If you only plan to do a small amount of work, it is better to jump into an interactive session rather than submit a job to request many resources. This type of job will usually start quickly.</p>
    <p>To request a 30-minute interactive session, use the following command:</p>
    <pre><code>sdev</code></pre>
    <p>This will allow you to start a quick and interactive session on the Sherlock cluster for shorter tasks or testing purposes.</p>
</body>
</html>

